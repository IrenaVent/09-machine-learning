{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  <span style=\"color:#5D8BF4\"> Irena Vent </span>\n",
    "\n",
    "# <span style=\"color:#051367\"> Machine Learning</span>\n",
    "\n",
    "<span style=\"color:#051367\"> **Dataset**: Airbnb </span>\n",
    "\n",
    "<span style=\"color:#051367\"> **Objetivo**: Predecir el precio de un airbnb situado Madrid</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "%matplotlib inline\n",
    "\n",
    "cm = plt.cm.RdBu\n",
    "cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(confmat):\n",
    "    fig, ax = plt.subplots(figsize=(7, 7))\n",
    "    ax.matshow(confmat, cmap=plt.cm.Blues, alpha=0.5)\n",
    "    for i in range(confmat.shape[0]):\n",
    "        for j in range(confmat.shape[1]):\n",
    "            ax.text(x=j, y=i, s=confmat[i, j], va='center', ha='center')\n",
    "\n",
    "    plt.xlabel('predicted label')\n",
    "    plt.ylabel('true label')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <span style=\"color:#5D8BF4\"> Cargamos nuestro dataset con el objetivo de: </span>\n",
    "    \n",
    "1. Filtrar el dataset por Madird (Spain); \n",
    "2. Comprobar si existen variables que puedan ser descartadas previo al análisis exploratior de datos, por ejemplo: ids, urls, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "prueba = pd.read_csv('./data/airbnb-listings.csv', sep=';', decimal='.')\n",
    "\n",
    "#pd.set_option('display.max_columns', None)\n",
    "#prueba.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14780, 89)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prueba.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13234, 40)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# eliminamos las muestras que contienen NaN en las columnas indicadas\n",
    "prueba = prueba.dropna(subset=['Country', 'City'])\n",
    "\n",
    "# filtramos por Spian y Madrid\n",
    "prueba = prueba.loc[prueba['Country'] == 'Spain']\n",
    "prueba = prueba[prueba[\"City\"].str.contains(\"Madrid\")]\n",
    "\n",
    "#print(f'Dimensión dataframe --> {prueba.shape}')\n",
    "#print(f'Valores únicos en Country --> {prueba[\"Country\"].unique()}')\n",
    "#print(f'Valores únicos en City --> {prueba[\"City\"].unique()}')\n",
    "\n",
    "# eliminamos columnas\n",
    "prueba = prueba.drop(['ID', 'Listing Url', 'Scrape ID', 'Last Scraped', 'Name', 'Summary', 'Space', 'Description',\n",
    "                      'Experiences Offered', 'Neighborhood Overview','Zipcode' , 'Notes', 'Transit', 'Access', \n",
    "                      'Interaction', 'House Rules', 'Thumbnail Url', 'Medium Url', 'Picture Url', 'XL Picture Url',\n",
    "                      'Host ID', 'Host URL', 'Host Name', 'Host Since', 'Host Location', 'Host About','Amenities', \n",
    "                      'Host Thumbnail Url', 'Host Picture Url', 'Host Neighbourhood','Host Listings Count', \n",
    "                      'Host Total Listings Count', 'Host Verifications', 'Street', 'City', 'State', 'Market',\n",
    "                      'Smart Location', 'Country Code', 'Country', 'First Review', 'Last Review', 'Calendar Updated', \n",
    "                      'Calendar last Scraped', 'License', 'Jurisdiction Names', 'Calculated host listings count',\n",
    "                      'Geolocation', 'Features'], axis = 1)\n",
    "\n",
    "prueba.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#5D8BF4\"> **Divsión del dataset en train y test** para el desarrollo de análisis exploratorio de datos. Que se realizará únicamente sobre el subconjunto de train. Posteriormente, en la validación del modelo elegido, las decisiones tomadas sobre train se aplicaran al subconjunto de test. </span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensiones del dataset de training: (10587, 40)\n",
      "Dimensiones del dataset de test: (2647, 40)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#full_df = pd.read_csv('./airbnb-listings-extract.csv', sep=';', decimal='.')\n",
    "train, test = train_test_split(prueba, test_size=0.2, shuffle=True, random_state=0)\n",
    "\n",
    "print(f'Dimensiones del dataset de training: {train.shape}')\n",
    "print(f'Dimensiones del dataset de test: {test.shape}')\n",
    "\n",
    "# Guardamos\n",
    "train.to_csv('./train.csv', sep=';', decimal='.', index=False)\n",
    "test.to_csv('./test.csv', sep=';', decimal='.', index=False)\n",
    "\n",
    "# A partir de este momento cargamos el dataset de train y trabajamos ÚNICAMENTE con él. \n",
    "df_train = pd.read_csv('./train.csv', sep=';', decimal='.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#5D8BF4\"> **Análisis exploratior de los datos** </span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "prueba.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_train.isnull().any()\n",
    "df_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#5D8BF4\"> **Primeras observaciones**</span>\n",
    "\n",
    "- Existen varibles, como son: Host Response Time, Neighbourhood, Neighbourhood Cleansed, Neighbourhood Group Cleansed, Property Type, Bed Type, Amenities, y Cancellation Policy, todas ellas de tipo **object** que deben ser transformadas y/o categorizadas;\n",
    "\n",
    "- Las variables Host Acceptance Rate y Has Availability pueden ser eliminadas al tener todas sus muestras ausentes; \n",
    "\n",
    "- La variable objetivo Price tiene 9 muestras con valores ausentes, que se eliminarán;\n",
    "       \n",
    "- La varieble Square Feet debe ser transformada a Square Meter observamos que más de 96% de sus muestras están ausentes, por lo que decidimos eliminar dicha columna;\n",
    "    \n",
    "- La varible Zipcode parece tener datos erróneos, reisar y eliminar valores ausente;\n",
    "    \n",
    "- Imputar los valores ausentes con la moda en Bed, Bedroom y Bathroom, el 75% de la muestra está dentro del rango, pues parece haber algún autlier;\n",
    "    \n",
    "- Sobre las variables Neighbourhood, Neighbourhood Cleansed y Neighbourhood Group Cleansed ver contenido de cada varaibles para decidir si es necesario eliminar alguna de ellas;\n",
    "    \n",
    "- Para las variables Weekly Price y Monthly Price analizar con un estudio de correlación y decidir si se pueden eliminar;\n",
    "    \n",
    "- Muchas de las variabbles (Accommodates, Square Feet, Weekly Price, Monthly Price, Security Deposit, Number of Reviews, etc) presentan valores dispares, por lo que sería necesario explorar los datos para ecnontrar posibles outliers;\n",
    "    \n",
    "- Todas las variables Review excepto Number of Reviews, presentan valores ausentes, la imputación según los datos, puede ser a través de las media o valor meas frecuente. Pero sería necesario valorar la opción de eliminar alguna de dichas variables con un estudio de correlaciones; </span>\n",
    "\n",
    "<span style=\"color:#5D8BF4\"> **Gráfico de correlaciones** valorar si podemos eliminar alguna variables más en este punto.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Compute the correlation matrix\n",
    "corr = np.abs(df_train.drop(['Price'], axis=1).corr())\n",
    "\n",
    "# Generate a mask for the upper triangle\n",
    "mask = np.zeros_like(corr, dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "f, ax = plt.subplots(figsize=(12, 10))\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(corr, mask=mask,vmin = 0.0, vmax=1.0, center=0.5,\n",
    "            linewidths=.1, cmap=\"YlGnBu\", cbar_kws={\"shrink\": .8})\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eliminamos las columnas con valores ausentes y alta correlación entre variables predictoras\n",
    "# evitar la colinealidad\n",
    "df_train = df_train.drop(['Host Acceptance Rate', 'Host Response Time', 'Has Availability', 'Square Feet', \n",
    "                          'Neighbourhood', 'Neighbourhood Cleansed', 'Weekly Price', 'Monthly Price', \n",
    "                          'Has Availability', 'Availability 30', 'Availability 60', 'Availability 365',\n",
    "                          'Review Scores Accuracy', 'Review Scores Checkin','Review Scores Communication', \n",
    "                          'Reviews per Month', 'Review Scores Value'], axis = 1)\n",
    "\n",
    "# eliminamos 9 muestras con Price NaN\n",
    "df_train = df_train.dropna(subset=['Price'])\n",
    "\n",
    "print(f'Porcentaje de muestras eliminadas: {round((10587-df_train.shape[0])/10587*100,4)} %')\n",
    "print(f'Tamaño df_train: {df_train.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#5D8BF4\"> Vamos a explorar las variables Property/Room/Bed Type y Cancellation Policy para ver su categorización.<span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['Room Type', 'Bed Type', 'Cancellation Policy', 'Property Type']\n",
    "\n",
    "for column in columns:\n",
    "    print (f'{column} --> {df_train[column].unique()}')\n",
    "    print (f'{column} --> {len(df_train[column].unique())}')\n",
    "    print (df_train[column].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#5D8BF4\"> Tomamos las siguientes decisiones:<span>\n",
    "\n",
    "- **Room Type** pasará a ser variable numérica con 3 valores;\n",
    "- **Bed Type** pasará a ser variable numérica con 3 valores, dado que las 2 primeros grupos explican el 99% de las muestras, las tres últimas pasaran a formar un sólo grupo;\n",
    "- **Cancellation Policy** pasará a ser variable numérica con 4 valores, super_strict_60 y super_strict_30 se van a agrupar en un único grupo, que representará al nuevo grupo super-strict;\n",
    "- **Property Type** pasará a ser variable numérica con 7 valores, dado que los 6 primeros grupos explican más del 98% de las muestras, es decir, el resto de grupos pasaran a formar un solo grupo el de new_other;\n",
    "    \n",
    "<span style=\"color:#5D8BF4\"> Aplicamos transformaciones:<span>\n",
    "    \n",
    "<span style=\"color:#5D8BF4\"> **Codificación de variables**. Decidimos usar *MeanEncoder* para codificar la variable Neighbourhood Group Cleansed y Property Type. *LeableEncoder* para Room, Bed and Cancellation Policy. </span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unificar grupos \n",
    "df_train['Bed Type'] = df_train['Bed Type'].str.replace('Futon', 'Other')\n",
    "df_train['Bed Type'] = df_train['Bed Type'].str.replace('Couch', 'Other')\n",
    "df_train['Bed Type'] = df_train['Bed Type'].str.replace('Airbed', 'Other')\n",
    "\n",
    "df_train['Cancellation Policy'] = df_train['Cancellation Policy'].str.replace('super_strict_60', 'super_strict')\n",
    "df_train['Cancellation Policy'] = df_train['Cancellation Policy'].str.replace('super_strict_30', 'super_strict')\n",
    "\n",
    "p_types = ['Chalet', 'Guesthouse', 'Dorm', 'Boutique hotel', 'Hostel', 'Serviced apartment',\n",
    "          'Townhouse', 'Guest suite', 'Earth House', 'Tent', 'Timeshare', 'Villa', 'Casa particular',\n",
    "          'Camper/RV', 'Bungalow', 'Boat']\n",
    "\n",
    "for p_type in p_types:\n",
    "    df_train['Property Type'] = df_train['Property Type'].str.replace(p_type, 'new_Other')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Codificación de la variables Neighbourhood Cleansed\n",
    "def calc_smooth_mean(df, by, on, m):\n",
    "    # Compute the global mean\n",
    "    mean = df_train[on].mean()\n",
    "\n",
    "    # Compute the number of values and the mean of each group\n",
    "    agg = df.groupby(by)[on].agg(['count', 'mean'])\n",
    "    counts = agg['count']\n",
    "    means = agg['mean']\n",
    "\n",
    "    # Compute the \"smoothed\" means\n",
    "    smooth = (counts * means + m * mean) / (counts + m)\n",
    "\n",
    "    # Replace each value by the according smoothed mean\n",
    "    return df[by].map(smooth)\n",
    "\n",
    "df_train['Neighbourhood Group Cleansed'] = calc_smooth_mean(df_train, by='Neighbourhood Group Cleansed', on='Price', m=df_train.shape[0])\n",
    "df_train['Property Type'] = calc_smooth_mean(df_train, by='Property Type', on='Price', m=df_train.shape[0])\n",
    "\n",
    "\n",
    "# Codificación de variables con LabelEncoder\n",
    "\n",
    "#le_prop_type = LabelEncoder()\n",
    "#le_prop_type.fit(df_train['Property Type'])\n",
    "#df_train['Property Type'] = le_prop_type.transform(df_train['Property Type'])\n",
    "\n",
    "le_hrr = LabelEncoder()\n",
    "le_hrr.fit(df_train['Host Response Rate'])\n",
    "df_train['Host Response Rate'] = le_hrr.transform(df_train['Host Response Rate'])\n",
    "\n",
    "le_room_type = LabelEncoder()\n",
    "le_room_type.fit(df_train['Room Type'])\n",
    "df_train['Room Type'] = le_room_type.transform(df_train['Room Type'])\n",
    "\n",
    "le_bed_type = LabelEncoder()\n",
    "le_bed_type.fit(df_train['Bed Type'])\n",
    "df_train['Bed Type'] = le_bed_type.transform(df_train['Bed Type'])\n",
    "\n",
    "le_cancell_p = LabelEncoder()\n",
    "le_cancell_p.fit(df_train['Cancellation Policy'])\n",
    "df_train['Cancellation Policy'] = le_cancell_p.transform(df_train['Cancellation Policy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#5D8BF4\"> **Imputación de valores ausentes** </span>\n",
    "\n",
    "- Tenemos un neumero bajo de variables a imputar por lo que decidimos tomar la decisión de imputar Bathrooms, Bedrooms y Beds con el valor meas frecuente, aun siendo valor numéricos, no categóricos, su valores puede tender a comportarse como tal. Usaremos mismo prámetros para las variebles Review;\n",
    "- Las varibles Security Deposit y Cleaning Fee tienen un alto porcetaje de valores perdidos, pero consideramos que ambas variables pueden ser variables predictoras, por ello no las eliminamos. Imputamos los valores perdidos con 0, asuminedo que estas muestras no aplican dichas tarifas;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imputer_mf = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n",
    "imputer_mf = imputer_mf.fit(df_train[['Bathrooms', 'Bedrooms', 'Beds', 'Review Scores Rating','Review Scores Cleanliness', 'Review Scores Location']])\n",
    "df_train = df_train.copy()\n",
    "df_train[['Bathrooms', 'Bedrooms', 'Beds', 'Review Scores Rating','Review Scores Cleanliness', 'Review Scores Location']] = imputer_mf.transform(df_train[['Bathrooms', 'Bedrooms', 'Beds', 'Review Scores Rating','Review Scores Cleanliness', 'Review Scores Location']])\n",
    "\n",
    "df_train['Security Deposit'] = df_train['Security Deposit'].fillna(0)\n",
    "df_train['Cleaning Fee'] = df_train['Cleaning Fee'].fillna(0)\n",
    "\n",
    "#imputer_mn = SimpleImputer(missing_values=np.nan, strategy='median')\n",
    "#imputer_mn = imputer_mn.fit(df_p[['Security Deposit', 'Cleaning Fee']])\n",
    "#df_p = df_p.copy()\n",
    "#df_p[['Security Deposit', 'Cleaning Fee']] = imputer_mn.transform(df_p[['Security Deposit', 'Cleaning Fee']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#5D8BF4\"> **Distribuciones, outliers y correlaciones (incluyendo Price)**. Veamos previamente cómo son las variables relaciones y posibles filtrados de los datos. </span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Compute the correlation matrix\n",
    "corr = np.abs(df_train.corr())\n",
    "\n",
    "# Generate a mask for the upper triangle\n",
    "mask = np.zeros_like(corr, dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "f, ax = plt.subplots(figsize=(12, 10))\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(corr, mask=mask,vmin = 0.0, vmax=1.0, center=0.5,\n",
    "            linewidths=.1, cmap=\"YlGnBu\", cbar_kws={\"shrink\": .8})\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#5D8BF4\"> Del greafico de correlaciones podemos extraer las siguientes observaiones: </span>\n",
    "\n",
    "- Las variables Beds y Accommodates parecen que tener una alta correlación (porsible colinealidad), por ahora decidimos no eliminar ninguna de ellas, dado que ambas tienen una buena correlación con la variables objetivo Price, la elminación de una u otra variable se realizará en base a un estudio más exaustiva;\n",
    "- Dicidimos eliminar las variables Minimun Nights, Maximum Nights, Avialability 90, Number of Reviews al presentar éstas una muy baja correlacieon con la varaible objetivo;\n",
    "- Ídem para la variable Bed Type;\n",
    "\n",
    "<span style=\"color:#5D8BF4\"> Eliminamos las variables </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.drop(['Host Response Rate', 'Minimum Nights', 'Maximum Nights',\n",
    "                          'Availability 90', 'Number of Reviews', 'Bed Type'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = df_train.columns\n",
    "plt.figure(figsize=(30, 30))\n",
    "\n",
    "for n, col in enumerate(columns):\n",
    "    plt.subplot(6,4,n+1)\n",
    "    df_train[col].plot.hist(grid = True)\n",
    "    plt.axis()\n",
    "    plt.xlabel(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['Accommodates', 'Bathrooms', 'Bedrooms', 'Beds', 'Security Deposit', \n",
    "           'Cleaning Fee', 'Price', 'Extra People']\n",
    "\n",
    "plt.figure(figsize=(20, 20))\n",
    "\n",
    "for n, col in enumerate(columns):\n",
    "    plt.subplot(3,3,n+1)\n",
    "    df_train.boxplot(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.plot(kind = 'scatter',x='Accommodates',y = 'Price')\n",
    "plt.xlabel('# Accommodates')\n",
    "plt.ylabel('Price (€)')\n",
    "plt.show()\n",
    "\n",
    "df_train.plot(kind = 'scatter',x='Bathrooms',y = 'Price')\n",
    "plt.xlabel('# Bathrooms')\n",
    "plt.ylabel('Price (€)')\n",
    "plt.show()\n",
    "\n",
    "df_train.plot(kind = 'scatter',x='Bedrooms',y = 'Price')\n",
    "plt.xlabel('# Bedrooms')\n",
    "plt.ylabel('Price (€)')\n",
    "plt.show()\n",
    "\n",
    "df_train.plot(kind = 'scatter',x='Beds',y = 'Price')\n",
    "plt.xlabel('# Beds')\n",
    "plt.ylabel('Price (€)')\n",
    "plt.show()\n",
    "\n",
    "df_train.plot(kind = 'scatter',x='Security Deposit',y = 'Price')\n",
    "plt.xlabel('# Security Deposit')\n",
    "plt.ylabel('Price (€)')\n",
    "plt.show()\n",
    "\n",
    "df_train.plot(kind = 'scatter',x='Extra People',y = 'Price')\n",
    "plt.xlabel('# Extra People')\n",
    "plt.ylabel('Price (€)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#5D8BF4\"> De los gráficos extraemos las siguientes observaiones: </span>\n",
    "\n",
    "- Debemos filtrar las variables Accommodates, Bathrooms, Bedrooms, Beds, Security Deposit, Cleaning Fee y Price;\n",
    "- La variables objetivo Price así como a las variables escoradas hacía los extremos, sería necesario aplicar una transformacieon logarítmica para normalizar sus distribuciones. Se aplicará el logaritmo a las variables cuyo rango está dentro del rango logarítmico, esto es, valores postivisos distintos de 0;\n",
    "\n",
    "<span style=\"color:#5D8BF4\"> Eliminación de outliers y aplicamos transformaciones</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_train['Accommodates'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train[df_train['Accommodates'] <= 12]\n",
    "df_train = df_train[df_train['Bathrooms'] <= 6]\n",
    "df_train = df_train[df_train['Bedrooms'] <= 6]\n",
    "df_train = df_train[df_train['Beds'] <= 10]\n",
    "df_train = df_train[df_train['Extra People'] <= 50]\n",
    "df_train = df_train[df_train['Security Deposit'] <= 600]\n",
    "df_train = df_train[df_train['Price'] <= 600]\n",
    "\n",
    "print(f'Porcentaje de muestras eliminadas: {round((10587-df_train.shape[0])/10587*100,4)} %')\n",
    "print(f'Tamaño df_train: {df_train.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transformación logarítmica de los datos\n",
    "columns = ['Accommodates', 'Price']\n",
    "\n",
    "for c in columns:\n",
    "    df_train[c] = df_train[c].apply(lambda x: np.log10(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['Accommodates', 'Price']\n",
    "plt.figure(figsize=(30, 30))\n",
    "\n",
    "for n, col in enumerate(columns):\n",
    "    plt.subplot(6,4,n+1)\n",
    "    df_train[col].plot.hist(grid = True)\n",
    "    plt.axis()\n",
    "    plt.xlabel(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#5D8BF4\">Movemos variable objetivo en primera posición.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_column = df_train.pop('Price')\n",
    "df_train.insert(0, \"Price\", first_column)\n",
    "\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#5D8BF4\">**Filtrado y selección de varaibles**</span>\n",
    "\n",
    "<span style=\"color:#5D8BF4\">Vamos a pasar a ver algunos métodos y comprobar la posibilidad de reducir el número de variables predictoras, para mejorar la intepretabilidad del modelo a desarrollar. Es también probable que con un número alto de variables predictoras es más fácil que el; modelo incurra en overfitting.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import f_regression, mutual_info_regression\n",
    "\n",
    "# convertimos el DataFrame al formato necesario para scikit-learn\n",
    "data = df_train.values \n",
    "\n",
    "y = data[:,0:1]     # nos quedamos con la 1ª columna, price\n",
    "X = data[:,1:]      # nos quedamos con el resto\n",
    "\n",
    "feature_names = df_train.columns[1:]\n",
    "\n",
    "# do calculations\n",
    "f_test, _ = f_regression(X, y)\n",
    "f_test /= np.max(f_test)\n",
    "\n",
    "mi = mutual_info_regression(X, y)\n",
    "mi /= np.max(mi)\n",
    "\n",
    "# do some plotting\n",
    "plt.figure(figsize=(20, 5))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.bar(range(X.shape[1]),f_test,  align=\"center\")\n",
    "plt.xticks(range(X.shape[1]),feature_names, rotation = 90)\n",
    "plt.xlabel('features')\n",
    "plt.ylabel('Ranking')\n",
    "plt.title('$F Test$ score')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.bar(range(X.shape[1]),mi, align=\"center\")\n",
    "plt.xticks(range(X.shape[1]),feature_names, rotation = 90)\n",
    "plt.xlabel('features')\n",
    "plt.ylabel('Ranking')\n",
    "plt.title('Mutual information score')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
